<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Bioinfor | 牛牛龙]]></title>
  <link href="http://yulongniu.bionutshell.org/blog/categories/bioinfor/atom.xml" rel="self"/>
  <link href="http://yulongniu.bionutshell.org/"/>
  <updated>2017-11-21T22:21:15+08:00</updated>
  <id>http://yulongniu.bionutshell.org/</id>
  <author>
    <name><![CDATA[Yulong Niu]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Bray-Curtis Distance解释]]></title>
    <link href="http://yulongniu.bionutshell.org/blog/2017/10/24/bray-curtis-distance/"/>
    <updated>2017-10-24T12:26:00+08:00</updated>
    <id>http://yulongniu.bionutshell.org/blog/2017/10/24/bray-curtis-distance</id>
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<script type="math/tex; mode=display">\newcommand{\sumup}[1] {\sum\limits_{i=1}^{n} #1}</script>

<p>Bray-Curtis distance（BCD）的定义为：</p>

<!--more-->

<script type="math/tex; mode=display">\begin{align}
\begin{split}
BCD(X, Y) = \frac{\sumup{|x_i - y_j|}}{\sumup{x_i} + \sumup{y_i}}
\end{split}
\label{eq:1}
\end{align}</script>

<p>其中，$X$和$Y$分别为长度为$n$的数值向量。根据$\eqref{eq:1}$可以得出：$BCD$的取值范围为$[0, 1]$；当$X$和$Y$完全相同时，$BCD$为0；反之，$BCD$为1。</p>

<p>同样，Bray-Curtis similarity（BCS）或Bray-Curtis index为：</p>

<script type="math/tex; mode=display">\begin{align}
\begin{split}
BCS(X, Y) = 1 - BCD(X, Y)
\end{split}
\label{eq:2}
\end{align}</script>

<h3 id="section">参考资料</h3>

<ul>
  <li><a href="http://84.89.132.1/~michael/stanford/maeb6.pdf">Chapter 6 Measures of distance and correlation between variables</a></li>
</ul>

<h3 id="section-1">更新记录</h3>

<p>2017年10月22日</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Normalized Google Distance解释]]></title>
    <link href="http://yulongniu.bionutshell.org/blog/2017/10/22/google-distance/"/>
    <updated>2017-10-22T21:35:30+08:00</updated>
    <id>http://yulongniu.bionutshell.org/blog/2017/10/22/google-distance</id>
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<script type="math/tex; mode=display">\newcommand{\sumup}[1] {\sum\limits_{i=1}^{n} #1}</script>

<p>本文尝试探索Normalized Google distance（简称NGD）的定义和拓展应用。</p>

<h3 id="ngd">1. NGD原始定义</h3>

<p><a href="https://en.wikipedia.org/wiki/Normalized_Google_distance">维基百科</a>的定义为：</p>

<!--more-->

<script type="math/tex; mode=display">\begin{align}
\begin{split}
NGD(x, y) = \frac{\max\{\log f(x), \log f(y)\} - \log f(x, y)}{\log N - \min\{\log f(x), \log f(y)\}}
\end{split}
\label{eq:1}
\end{align}</script>

<p>其中，$f(x)$和$f(y)$分别为关键词$x$和$y$出现的次数，$f(x,y)$为$x$和$y$同时出现的次数，$N$为全部搜索单词数目。根据$\eqref{eq:1}$可以得出：如果$x$和$y$几乎总是同时出现时，$NGD$趋近于$0$；如果$x$和$y$出现的次数很少，即$\log f(x,y)$趋近于负无穷，则$NGD$可能大于$1$。</p>

<h3 id="ngd-1">2. NGD定义延伸</h3>

<p>Choi and Rashid在2008年的文章（参考资料1）提出一种针对向量的$NGD$定义：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{align}
\begin{split}
NGD(X, Y) &= \frac{\max\left\{\sumup{x_i}, \sumup{y_i}\right\} - \sumup{\min(x_i, y_i)}}{\sumup{x_i} + \sumup{y_i} - \sumup{\min(x_i, y_i)}} \\
&= \frac{\max\left\{\sumup{x_i}, \sumup{y_i}\right\} - \sumup{\min(x_i, y_i)}}{\max\left\{\sumup{x_i}, \sumup{y_i}\right\}}
\end{split}
\label{eq:2}
\end{align} %]]&gt;</script>

<p>其中，$X$和$Y$分别为长度为$n$的数值向量，$\min(x_i, y_i)$为$x_i$和$y_i$中各个元素最小值所组成的数值向量。根据$\eqref{eq:2}$可以得出：$NGD$的取值范围为$[0, 1]$；当$X$和$Y$完全相同时，$NGD$为0；反之，$NGD$为1。</p>

<p>由此，可以得到normalized Google similarity（NGS）为：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{align}
\begin{split}
NGS(X, Y) &= 1 - NGD(X, Y) \\
&= \frac{\sumup{\min(x_i, y_i)}}{\max\left\{\sumup{x_i}, \sumup{y_i}\right\}}
\end{split}
\label{eq:3}
\end{align} %]]&gt;</script>

<p>一个例子：$X = (1, 2, 0, 3)$和$Y = (0, 2, 1, 1)$，则$NGD = 0.5$和$NGS = 0.5$。</p>

<h3 id="section">参考资料</h3>

<ul>
  <li><a href="https://www.cs.cmu.edu/afs/cs/user/aberger/www/html/tutorial/tutorial.html">Adapting Normalized Google Similarity in Protein Sequence Comparison</a></li>
</ul>

<h3 id="section-1">更新记录</h3>

<p>2017年10月21日</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[最大熵模型]]></title>
    <link href="http://yulongniu.bionutshell.org/blog/2017/10/16/max-entropy/"/>
    <updated>2017-10-16T13:16:12+08:00</updated>
    <id>http://yulongniu.bionutshell.org/blog/2017/10/16/max-entropy</id>
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<h3 id="section">1. 熵和条件熵</h3>

<p>对于随机变量$X$，熵为：</p>

<script type="math/tex; mode=display">\begin{align}
\begin{split}
H(X) = -\sum_{x \in X}p(x)\log{p(x)}
\end{split}
\label{eq:1}
\end{align}</script>

<!--more-->

<p>其中：</p>

<script type="math/tex; mode=display">\begin{align}
\begin{split}
\sum_{x \in X}p(x) = 1
\end{split}
\label{eq:2}
\end{align}</script>

<p>同样道理，对于任意随机变量$X$和$Y$，联合熵为：</p>

<script type="math/tex; mode=display">\begin{align}
\begin{split}
H(X,Y) = -\sum_{x \in X, y \in Y}p(x,y)\log{p(x,y)}
\end{split}
\label{eq:3}
\end{align}</script>

<p>基于$X$的$Y$的熵为条件熵：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{align}
\begin{split}
H(Y|X) &= H(X, Y) - H(X) \\
&= -\sum_{x \in X, y \in Y}p(x,y)\log{p(x,y)} + \sum_{x \in X}p(x)\log{p(x)} \\
&= -\sum_{x \in X, y \in Y}p(x,y)\log{p(x,y)} + \sum_{x \in X, y \in Y}p(x, y)\log{p(x)} \\
&= -\sum_{x \in X, y \in Y}p(x, y)\log \frac{p(x, y)}{p(x)}
\end{split}
\label{eq:4}
\end{align} %]]&gt;</script>

<h3 id="section-1">2. 最大熵原理简介</h3>

<p>最大熵原理可以表述为，在满足$k+1$个约束条件的模型集合中，选取熵$H(p)$最大的模型。约束条件为：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{align}
\begin{split}
\sum_{x}p(x) &= 1 \\
\sum_{x}p(x)f_1(x) &= \tau_1 \\
\vdots \\
\sum_{x}p(x)f_k(x) &= \tau_k
\end{split}
\label{eq:5}
\end{align} %]]&gt;</script>

<p>使用拉格朗日乘子法求解带上述有约束的极值，即：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{align*}
\begin{split}
L(p) = -\sum_{x}p(x)\log{p(x)} &+ \\
\lambda_0(\sum_{x}p(x) - 1) &+ \\
\lambda_1(\sum_{x}p(x)f_1(x) - \tau_1) &+ \\
\cdots &+\\
\lambda_k(\sum_{x}p(x)f_1(x) - \tau_k)
\end{split}
\end{align*} %]]&gt;</script>

<p>$L(p)$对每一个$p(x)$偏导数$\frac{\partial L(p)}{\partial p(x)}$为0，即：</p>

<script type="math/tex; mode=display">-\log{p(x)} - 1 + \lambda_0 + \sum_{j=1}^{k}\lambda_j f_j(x) = 0</script>

<p>解得</p>

<script type="math/tex; mode=display">p(x) = \frac{\exp\left(\sum\limits_{j=1}^{k}\lambda_j f_j(x)\right)}{\exp(1 - \lambda_0)}</script>

<p>由约束条件$\sum\limits_x p(x)=1$得：</p>

<script type="math/tex; mode=display">\begin{align}
p(x) = \frac{1}{Z}\exp\left(\sum\limits_{j=1}^{k}\lambda_j f_j(x)\right)
\label{eq:6}
\end{align}</script>

<p>其中</p>

<script type="math/tex; mode=display">\begin{align}
Z = \sum\limits_x \exp\left(\sum\limits_{j=1}^{k}\lambda_j f_j(x)\right)
\label{eq:7}
\end{align}</script>

<p>将$\eqref{eq:3}$带入约束条件$\eqref{eq:2}$中，即可解得$\lambda_j$。</p>

<h3 id="section-2">3. 最大熵应用例子</h3>

<p>根据参考资料2的例子，应用最大熵模型。例子简述为：</p>

<blockquote>
  <p>三种食物的售价分别为1、2和3元，平均一餐花费1.75元。</p>

  <p>估算每种食物被购买的概率。</p>
</blockquote>

<p>建立最大熵模型：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{align*}
\begin{split}
max \quad & H(p) = \sum_{x}p(x) \log p(x) \\
s.t. \quad & \sum_{x}p(x)x = 1.75 \\
\quad & \sum_{x}p(x) = 1
\end{split}
\end{align*} %]]&gt;</script>

<p>解方程得：$p(x_1) = 0.466$、 $p(x_2) = 0.318$、$p(x_3) = 0.216$</p>

<h3 id="section-3">参考资料</h3>

<ul>
  <li>
    <p><a href="https://www.cs.cmu.edu/afs/cs/user/aberger/www/html/tutorial/tutorial.html">A Brief Maxent Tutorial</a></p>
  </li>
  <li>
    <p><a href="http://www-mtl.mit.edu/Courses/6.050/2003/notes/chapter9.pdf">Chapter 9 Principle of Maximum Entropy: Simple Form</a></p>
  </li>
  <li>
    <p><a href="http://spaces.ac.cn/archives/3552/">“熵”不起：从熵、最大熵原理到最大熵模型</a></p>
  </li>
</ul>

<h3 id="section-4">更新记录</h3>

<p>2017年7月15日</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[朴素贝叶斯分类器应用于二元数据类型]]></title>
    <link href="http://yulongniu.bionutshell.org/blog/2017/10/15/naive-bayes-binary-multinomial/"/>
    <updated>2017-10-15T17:38:16+08:00</updated>
    <id>http://yulongniu.bionutshell.org/blog/2017/10/15/naive-bayes-binary-multinomial</id>
    <content type="html"><![CDATA[<script type="text/x-mathjax-config">
MathJax.Hub.Config({
TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

<h3 id="section">1. 贝叶斯定理</h3>

<p>已知事件$A$和$B$，则条件概率为：</p>

<!--more-->

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{align}
\begin{split}
P(A | B) &= \frac{P(A,B)}{P(B)} \\
P(B | A) &= \frac{P(A,B)}{P(A)}
\end{split}
\label{eq:1}
\end{align} %]]&gt;</script>

<p>可以推导出：</p>

<script type="math/tex; mode=display">\begin{align}
P(B|A) = \frac{P(A|B)P(B)}{P(A)}
\label{eq:2}
\end{align}</script>

<h3 id="section-1">2. 分类器简介</h3>

<p>朴素贝叶斯分类器（Naive Bayes classifier）是一种简单、有效的分类器，其难点在于估算条件概率。比如，一个数据集拥有$N$个相互独立的特征，$C$个分组，对于$C_j$条件概率模型为：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{align}
\begin{split}
p(C_j|F_1,\cdots,F_n) &= \frac{p(F_1,\cdots,F_n|C_j)p(C_j)}{p(F_1,\cdots,F_n)} \\
&= p(F_1|C_j) \cdots p(F_n|C_j)p(C_j)(1/p(F_1,\cdots,F_n))
\end{split}
\label{eq:3}
\end{align} %]]&gt;</script>

<p>由于$1/p(F_1,\cdots,F_n)$在不同分组中为定值，因此：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{align}
\begin{split}
p(C_j|F_1,\cdots,F_n) &\propto p(C_j)\prod_{i=1}^{N}p(F_i|C_j)
\end{split}
\label{eq:4}
\end{align} %]]&gt;</script>

<p>其中，$p(C_j)$通常容易求得，即$C_j$分组在测试数据集中出现的频率。而$p(F_i\ \vert C_j)$则根据不同的测试数据类型，有不同的估计值。</p>

<p>以下讨论两种二元数据类型，例如某个数据集有三种特征量：</p>

<script type="math/tex; mode=display">F = 
\left[
\begin{array}{f}
F_1\\
F_2\\
F_3
\end{array}
\right]</script>

<h3 id="section-2">3. 伯努利分布</h3>

<p>每一个特征量的取值都为$0$或$1$。分组$C_j$含有两个已知样本为：</p>

<script type="math/tex; mode=display">C_{j1} = 
\left[
\begin{array}{cj1}
0\\
1\\
0
\end{array}
\right]</script>

<script type="math/tex; mode=display">C_{j2} = 
\left[
\begin{array}{cj2}
0\\
0\\
1
\end{array}
\right]</script>

<p>某个预测样本为：</p>

<script type="math/tex; mode=display">C_{jp1} = 
\left[
\begin{array}{cjp1}
1\\
0\\
1
\end{array}
\right]</script>

<p>由于$p(F_i \vert C_j)$不能为0，根据<a href="https://en.wikipedia.org/wiki/Rule_of_succession">Rule of succession</a>得各个特征的条件概率为：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{align*}
\begin{split}
p(F1|C_j) &= \frac{0+1}{2+2} &= 1/4 \\
p(F2|C_j) &= \frac{1+1}{2+2} &= 1/2 \\
p(F3|C_j) &= \frac{1+1}{2+2} &= 1/2
\end{split}
\end{align*} %]]&gt;</script>

<h3 id="section-3">4. 二项分布</h3>

<p>每一个特征量的取值都一个元素为$0$或$1$的向量（长度可不等）。分组$C_j$含有两个已知样本为：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
C_{j1} = 
\left[
\begin{array}{cj1}
0 & 1 & 0 & 1\\
1 & 0 & 1\\
0 & 0
\end{array}
\right] %]]&gt;</script>

<script type="math/tex; mode=display">% &lt;![CDATA[
C_{j2} = 
\left[
\begin{array}{cj2}
0 & 1 & 1 & 1\\
1 & 1 & 1\\
0 & 0
\end{array}
\right] %]]&gt;</script>

<p>某个预测样本为：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
C_{jp1} = 
\left[
\begin{array}{cjp1}
0 & 0 & 1 & 1\\
1 & 0 & 1\\
0 & 0
\end{array}
\right] %]]&gt;</script>

<p>各个特征的条件git pull origin master概率为：</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\begin{align*}
\begin{split}
p(F1|C_j) &= \left(\frac{3+1}{8+2}\right)^2 \times \left(\frac{5+1}{8+2}\right)^2 \\
p(F2|C_j) &= \left(\frac{5+1}{6+2}\right)^2 \times \left(\frac{1+1}{6+2}\right) \\
p(F3|C_j) &= \left(\frac{4+1}{4+2}\right)^2
\end{split}
\end{align*} %]]&gt;</script>

<h3 id="section-4">优化</h3>

<ol>
  <li>
    <p>当特征较多时，会面临多个小数（$p$值）相乘。可以取对数后再相加，即$\sum\log{p}$。</p>
  </li>
  <li>
    <p>虽然上文讨论的是二元数据，但是朴素贝叶斯分类器也适用于连续或者离散数据类型。</p>
  </li>
</ol>

<h3 id="section-5">参考资料</h3>

<ul>
  <li>
    <p><a href="https://www.inf.ed.ac.uk/teaching/courses/inf2b/learnnotes/inf2b-learn-note07-2up.pdf">Text Classification using Naive Bayes</a></p>
  </li>
  <li>
    <p><a href="https://nlp.stanford.edu/IR-book/html/htmledition/naive-bayes-text-classification-1.html">Naive Bayes text classification</a></p>
  </li>
</ul>

<h3 id="section-6">更新记录</h3>

<p>2017年7月15日</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HISAT2使用问答]]></title>
    <link href="http://yulongniu.bionutshell.org/blog/2016/09/10/hisat2-faq/"/>
    <updated>2016-09-10T16:32:47+08:00</updated>
    <id>http://yulongniu.bionutshell.org/blog/2016/09/10/hisat2-faq</id>
    <content type="html"><![CDATA[<p><a href="https://ccb.jhu.edu/software/hisat2/manual.shtml#the-hisat2-build-indexer">HISAT2</a>是一款用于“对应（map）”二代测序数据（全基因组、转录组和外显子组）至目标基因组的工具，用来替代<a href="http://ccb.jhu.edu/software/hisat/index.shtml">HISAT</a>和<a href="http://www.ccb.jhu.edu/software/tophat/index.shtml">TopHat2</a>。本文汇集一些使用HISAT2的常见问题问答。</p>

<h2 id="hisat2">1. 如何解读HISAT2的输出统计？</h2>

<p>一个常见的双端测序样本HISAT2输出统计：</p>

<!--more-->

<p>
<code>
10000 reads; of these:
  10000 (100.00%) were paired; of these:
    650 (6.50%) aligned concordantly 0 times
    8823 (88.23%) aligned concordantly exactly 1 time
    527 (5.27%) aligned concordantly &gt;1 times
    ----
    650 pairs aligned concordantly 0 times; of these:
      34 (5.23%) aligned discordantly 1 time
    ----
    616 pairs aligned 0 times concordantly or discordantly; of these:
      1232 mates make up the pairs; of these:
        660 (53.57%) aligned 0 times
        571 (46.35%) aligned exactly 1 time
        1 (0.08%) aligned &gt;1 times
96.70% overall alignment rate
</code>
</p>

<ul>
  <li>
    <p>总共10000对reads；</p>
  </li>
  <li>
    <p>8823对concordant pairs（一对既方向匹配又有合适距离的reads）有1次精确比对；527对concordant pairs有1次以上比对；34对disconcordant pairs;</p>
  </li>
  <li>
    <p>616对不是concordant pairs，也不是disconcordant pairs中，571个reads有1次精确比对；1个read有1次以上比对；660个reads没有比对成功。</p>
  </li>
  <li>
    <p>因此，整体比对率为1 - (660 / 2) / 10000</p>
  </li>
</ul>

<h2 id="hisat2-1">2. 使用HISAT2前，是否需要对原始数据进行清洗？</h2>

<p>需要。</p>

<h3 id="section">更新记录</h3>

<p>2016年9月10日</p>
]]></content>
  </entry>
  
</feed>
